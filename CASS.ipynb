{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import KFold\n",
    "from torchvision import transforms as tsfm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from torchcontrib.optim import SWA\n",
    "from torchmetrics import Metric\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # data path\n",
    "    train_csv_path = '/scratch/ps4364/BTMRI/data/train.csv'\n",
    "    train_imgs_dir = '/scratch/ps4364/BTMRI/data/Training/'\n",
    "    # model info\n",
    "    # label info\n",
    "    label_num2str = {0: 'glioma',\n",
    "                     1: 'pituitary',\n",
    "                     2:'notumor',\n",
    "                     3:'meningioma'\n",
    "                     }\n",
    "    label_str2num = {'glioma': 0,\n",
    "                     'pituitary':1,\n",
    "                     'notumor':2,\n",
    "                     'meningioma':3\n",
    "                     }\n",
    "    fl_alpha = 1.0  # alpha of focal_loss\n",
    "    fl_gamma = 2.0  # gamma of focal_loss\n",
    "    cls_weight =  [0.2, 0.5970802919708029, 1.0, 0.25255474452554744]\n",
    "    cnn_name='resnet50'\n",
    "    vit_name='vit_base_patch16_384'\n",
    "    seed = 77\n",
    "    num_classes = 4\n",
    "    batch_size = 16\n",
    "    t_max = 16\n",
    "    lr = 1e-3\n",
    "    min_lr = 1e-6\n",
    "    n_fold = 6\n",
    "    num_workers = 8\n",
    "    gpu_idx = 0\n",
    "    device = torch.device(f'cuda:{gpu_idx}' if torch.cuda.is_available() else 'cpu')\n",
    "    gpu_list = [gpu_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Array =  [1321, 1457, 1595, 1339]\n",
      "Normalized Array =  [0.2, 0.5970802919708029, 1.0, 0.25255474452554744]\n"
     ]
    }
   ],
   "source": [
    "def normalize(arr, t_min, t_max):\n",
    "    norm_arr = []\n",
    "    diff = t_max - t_min\n",
    "    diff_arr = max(arr) - min(arr)\n",
    "    for i in arr:\n",
    "        temp = (((i - min(arr))*diff)/diff_arr) + t_min\n",
    "        norm_arr.append(temp)\n",
    "    return norm_arr\n",
    "  \n",
    "# assign array and range\n",
    "array_1d = [1321,1457,1595,1339]\n",
    "range_to_normalize = (0.2, 1)\n",
    "normalized_array_1d = normalize(\n",
    "    array_1d, range_to_normalize[0], \n",
    "  range_to_normalize[1])\n",
    "  \n",
    "# display original and normalized array\n",
    "print(\"Original Array = \", array_1d)\n",
    "print(\"Normalized Array = \", normalized_array_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 77\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed_everything(77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define train & valid image transformation\n",
    "\"\"\"\n",
    "DATASET_IMAGE_MEAN = (0.485, 0.456, 0.406)\n",
    "DATASET_IMAGE_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "train_transform = tsfm.Compose([tsfm.Resize((384,384)),\n",
    "                                tsfm.RandomApply([tsfm.ColorJitter(0.2, 0.2, 0.2),tsfm.RandomPerspective(distortion_scale=0.2),], p=0.3),\n",
    "                                tsfm.RandomApply([tsfm.RandomAffine(degrees=10),], p=0.3),\n",
    "                                tsfm.RandomVerticalFlip(p=0.3),\n",
    "                                tsfm.RandomHorizontalFlip(p=0.3),\n",
    "                                tsfm.ToTensor(),\n",
    "                                tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD), ])\n",
    "\n",
    "valid_transform = tsfm.Compose([tsfm.Resize((384,384)),\n",
    "                                tsfm.ToTensor(),\n",
    "                                tsfm.Normalize(DATASET_IMAGE_MEAN, DATASET_IMAGE_STD), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define dataset class\n",
    "\"\"\"\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, cfg, img_names: list, labels: list, transform=None):\n",
    "        self.img_dir = cfg.train_imgs_dir\n",
    "        self.img_names = img_names\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_names[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_ts = self.transform(img)\n",
    "        label_ts = self.labels[idx]\n",
    "        return img_ts, label_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define Focal-Loss\n",
    "\"\"\"\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    The focal loss for fighting against class-imbalance\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1e-12  # prevent training from Nan-loss error\n",
    "        self.cls_weights = torch.tensor([CFG.cls_weight],dtype=torch.float, requires_grad=False, device=CFG.device)\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        logits & target should be tensors with shape [batch_size, num_classes]\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(logits)\n",
    "        one_subtract_probs = 1.0 - probs\n",
    "        # add epsilon\n",
    "        probs_new = probs + self.epsilon\n",
    "        one_subtract_probs_new = one_subtract_probs + self.epsilon\n",
    "        # calculate focal loss\n",
    "        log_pt = target * torch.log(probs_new) + (1.0 - target) * torch.log(one_subtract_probs_new)\n",
    "        pt = torch.exp(log_pt)\n",
    "        focal_loss = -1.0 * (self.alpha * (1 - pt) ** self.gamma) * log_pt\n",
    "        focal_loss = focal_loss * self.cls_weights\n",
    "        return torch.mean(focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define F1 score metric\n",
    "\"\"\"\n",
    "class MyF1Score(Metric):\n",
    "    def __init__(self, cfg, threshold: float = 0.5, dist_sync_on_step=False):\n",
    "        super().__init__(dist_sync_on_step=dist_sync_on_step)\n",
    "        self.cfg = cfg\n",
    "        self.threshold = threshold\n",
    "        self.add_state(\"tp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fp\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fn\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "\n",
    "    def update(self, preds: torch.Tensor, target: torch.Tensor):\n",
    "        assert preds.shape == target.shape\n",
    "        preds_str_batch = self.num_to_str(torch.sigmoid(preds))\n",
    "        target_str_batch = self.num_to_str(target)\n",
    "        tp, fp, fn = 0, 0, 0\n",
    "        for pred_str_list, target_str_list in zip(preds_str_batch, target_str_batch):\n",
    "            for pred_str in pred_str_list:\n",
    "                if pred_str in target_str_list:\n",
    "                    tp += 1\n",
    "                if pred_str not in target_str_list:\n",
    "                    fp += 1\n",
    "\n",
    "            for target_str in target_str_list:\n",
    "                if target_str not in pred_str_list:\n",
    "                    fn += 1\n",
    "        self.tp += tp\n",
    "        self.fp += fp\n",
    "        self.fn += fn\n",
    "\n",
    "    def compute(self):\n",
    "        f1 = 2.0 * self.tp / (2.0 * self.tp + self.fn + self.fp)\n",
    "        return f1\n",
    "    \n",
    "    def num_to_str(self, ts: torch.Tensor) -> list:\n",
    "        batch_bool_list = (ts > self.threshold).detach().cpu().numpy().tolist()\n",
    "        batch_str_list = []\n",
    "        for one_sample_bool in batch_bool_list:\n",
    "            lb_str_list = [self.cfg.label_num2str[lb_idx] for lb_idx, bool_val in enumerate(one_sample_bool) if bool_val]\n",
    "            batch_str_list.append(lb_str_list)\n",
    "        return batch_str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DF = pd.read_csv(CFG.train_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>path</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>/scratch/ps4364/BTMRI/data/Training/glioma/Tr-...</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/scratch/ps4364/BTMRI/data/Training/glioma/Tr-...</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/scratch/ps4364/BTMRI/data/Training/glioma/Tr-...</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/scratch/ps4364/BTMRI/data/Training/glioma/Tr-...</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/scratch/ps4364/BTMRI/data/Training/glioma/Tr-...</td>\n",
       "      <td>glioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5707</th>\n",
       "      <td>5707</td>\n",
       "      <td>/scratch/ps4364/BTMRI/data/Training/meningioma...</td>\n",
       "      <td>meningioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5708</th>\n",
       "      <td>5708</td>\n",
       "      <td>/scratch/ps4364/BTMRI/data/Training/meningioma...</td>\n",
       "      <td>meningioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5709</th>\n",
       "      <td>5709</td>\n",
       "      <td>/scratch/ps4364/BTMRI/data/Training/meningioma...</td>\n",
       "      <td>meningioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5710</th>\n",
       "      <td>5710</td>\n",
       "      <td>/scratch/ps4364/BTMRI/data/Training/meningioma...</td>\n",
       "      <td>meningioma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5711</th>\n",
       "      <td>5711</td>\n",
       "      <td>/scratch/ps4364/BTMRI/data/Training/meningioma...</td>\n",
       "      <td>meningioma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5712 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               path  \\\n",
       "0              0  /scratch/ps4364/BTMRI/data/Training/glioma/Tr-...   \n",
       "1              1  /scratch/ps4364/BTMRI/data/Training/glioma/Tr-...   \n",
       "2              2  /scratch/ps4364/BTMRI/data/Training/glioma/Tr-...   \n",
       "3              3  /scratch/ps4364/BTMRI/data/Training/glioma/Tr-...   \n",
       "4              4  /scratch/ps4364/BTMRI/data/Training/glioma/Tr-...   \n",
       "...          ...                                                ...   \n",
       "5707        5707  /scratch/ps4364/BTMRI/data/Training/meningioma...   \n",
       "5708        5708  /scratch/ps4364/BTMRI/data/Training/meningioma...   \n",
       "5709        5709  /scratch/ps4364/BTMRI/data/Training/meningioma...   \n",
       "5710        5710  /scratch/ps4364/BTMRI/data/Training/meningioma...   \n",
       "5711        5711  /scratch/ps4364/BTMRI/data/Training/meningioma...   \n",
       "\n",
       "          target  \n",
       "0         glioma  \n",
       "1         glioma  \n",
       "2         glioma  \n",
       "3         glioma  \n",
       "4         glioma  \n",
       "...          ...  \n",
       "5707  meningioma  \n",
       "5708  meningioma  \n",
       "5709  meningioma  \n",
       "5710  meningioma  \n",
       "5711  meningioma  \n",
       "\n",
       "[5712 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Split train & validation into Cross-Validation Folds\n",
    "\"\"\"\n",
    "\n",
    "all_img_names: list = TRAIN_DF[\"path\"].values.tolist()\n",
    "all_img_labels: list = TRAIN_DF[\"target\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg=CFG()\n",
    "all_img_labels_ts = []\n",
    "for tmp_lb in all_img_labels:\n",
    "    tmp_label = torch.zeros([CFG.num_classes], dtype=torch.float)\n",
    "    place=cfg.label_str2num.get(tmp_lb)\n",
    "    k=int(place)\n",
    "    tmp_label[k] = 1.0\n",
    "    all_img_labels_ts.append(tmp_label)\n",
    "    \n",
    "k_fold = KFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object _BaseKFold.split at 0x1511a8db6900>\n"
     ]
    }
   ],
   "source": [
    "print(k_fold.split(all_img_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5712"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_img_labels_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_idx, (train_indices, valid_indices) in enumerate(k_fold.split(all_img_names)):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fold_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "cfg=CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU()\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "cfg=CFG()\n",
    "model_cnn = timm.create_model(cfg.cnn_name, pretrained=True)\n",
    "model_vit = timm.create_model(cfg.vit_name, pretrained=True)\n",
    "model_cnn.to(device)\n",
    "model_vit.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssl_train_model(train_loader,model_vit,criterion_vit,optimizer_vit,scheduler_vit,model_cnn,criterion_cnn,optimizer_cnn,scheduler_cnn,num_epochs):\n",
    "    writer = SummaryWriter()\n",
    "    phase = 'train'\n",
    "    model_cnn.train()\n",
    "    model_vit.train()\n",
    "    f1_score_cnn=0\n",
    "    f1_score_vit=0\n",
    "    for i in tqdm(range(num_epochs)):\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            for img,_ in train_loader:\n",
    "                f1_score_cnn=0\n",
    "                f1_score_vit=0\n",
    "                img = img.to(device)\n",
    "                pred_vit = model_vit(img)\n",
    "                pred_cnn = model_cnn(img)\n",
    "                model_sim_loss=loss_fn(pred_vit,pred_cnn)\n",
    "                loss = model_sim_loss.mean()\n",
    "                loss.backward()\n",
    "                optimizer_cnn.step()\n",
    "                optimizer_vit.step()\n",
    "                scheduler_cnn.step()\n",
    "                scheduler_vit.step()\n",
    "            print('For -',i,'Loss:',loss) \n",
    "            writer.add_scalar(\"Self-Supervised Loss/train\", loss, i)\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_cnn = SWA(torch.optim.Adam(model_cnn.parameters(), lr= 1e-3))\n",
    "optimizer_vit = SWA(torch.optim.Adam(model_vit.parameters(), lr= 1e-3))\n",
    "scheduler_cnn = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_cnn,\n",
    "                                                                    T_max=16,\n",
    "                                                                    eta_min=1e-6)\n",
    "scheduler_vit = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_vit,\n",
    "                                                                    T_max=16,\n",
    "                                                                    eta_min=1e-6)\n",
    "\n",
    "\n",
    "fl_alpha = 1.0  # alpha of focal_loss\n",
    "fl_gamma = 2.0  # gamma of focal_loss\n",
    "cls_weight = [0.9475164011246484, 0.4934395501405811, 0.5029053420805999, 0.2, 1.0]\n",
    "criterion_vit = FocalLoss(fl_alpha, fl_gamma)\n",
    "criterion_cnn = FocalLoss(fl_alpha, fl_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(x, y):\n",
    "    x =  torch.nn.functional.normalize(x, dim=-1, p=2)\n",
    "    y =  torch.nn.functional.normalize(y, dim=-1, p=2)\n",
    "    return 2 - 2 * (x * y).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[   0    1    2 ... 5709 5710 5711]\n",
      "[  28   34   38   42   43   44   48   49   51   65   75   77   87   92\n",
      "  100  102  105  130  131  133  134  139  140  155  161  168  171  178\n",
      "  182  185  190  195  197  204  216  228  253  260  267  291  297  299\n",
      "  303  307  310  313  318  329  335  339  344  347  356  361  363  366\n",
      "  377  387  397  402  408  410  413  414  420  441  443  444  448  450\n",
      "  451  464  467  468  476  480  487  489  497  498  499  515  524  525\n",
      "  528  534  545  548  549  558  563  566  568  595  600  605  606  610\n",
      "  612  616  624  627  629  632  634  635  636  637  665  666  679  680\n",
      "  702  706  708  710  720  722  732  750  756  767  771  773  774  780\n",
      "  786  796  797  803  810  812  813  822  825  830  836  846  848  861\n",
      "  862  866  867  871  873  875  888  891  892  894  899  912  919  921\n",
      "  923  925  929  932  934  938  944  960  961  964  965  971  975  977\n",
      "  981  985  993  996  998 1002 1003 1004 1005 1006 1007 1017 1018 1021\n",
      " 1028 1047 1052 1055 1060 1061 1075 1079 1084 1089 1104 1106 1112 1113\n",
      " 1115 1117 1121 1122 1126 1139 1144 1150 1165 1167 1168 1172 1173 1178\n",
      " 1179 1198 1210 1215 1225 1231 1234 1239 1249 1255 1273 1274 1282 1285\n",
      " 1287 1288 1301 1310 1311 1322 1324 1325 1326 1327 1333 1334 1336 1338\n",
      " 1342 1345 1346 1349 1357 1358 1360 1361 1367 1372 1373 1375 1381 1383\n",
      " 1403 1407 1413 1419 1425 1437 1447 1462 1477 1478 1482 1489 1494 1495\n",
      " 1508 1517 1530 1549 1552 1558 1561 1564 1565 1566 1572 1589 1621 1624\n",
      " 1634 1640 1649 1651 1683 1700 1706 1707 1712 1718 1720 1725 1737 1747\n",
      " 1751 1764 1765 1769 1773 1776 1780 1786 1795 1803 1806 1807 1811 1812\n",
      " 1814 1820 1827 1828 1843 1854 1856 1864 1871 1872 1880 1896 1900 1901\n",
      " 1905 1918 1919 1928 1930 1935 1938 1947 1959 1960 1963 1965 1968 1969\n",
      " 1978 1984 1990 1993 2021 2026 2027 2047 2053 2054 2067 2070 2074 2082\n",
      " 2101 2104 2107 2118 2119 2126 2127 2138 2146 2148 2157 2159 2160 2161\n",
      " 2166 2169 2176 2183 2185 2187 2190 2191 2193 2196 2210 2213 2214 2216\n",
      " 2217 2219 2224 2226 2233 2234 2242 2245 2250 2252 2257 2258 2261 2263\n",
      " 2269 2275 2287 2302 2309 2312 2314 2315 2318 2327 2331 2333 2334 2335\n",
      " 2338 2342 2346 2347 2349 2355 2362 2378 2379 2387 2389 2393 2394 2402\n",
      " 2410 2412 2415 2419 2428 2429 2430 2433 2436 2448 2455 2459 2462 2465\n",
      " 2470 2476 2477 2485 2486 2500 2503 2515 2518 2526 2528 2530 2544 2545\n",
      " 2548 2550 2560 2563 2565 2570 2574 2577 2586 2589 2590 2594 2601 2602\n",
      " 2604 2608 2614 2616 2620 2633 2634 2636 2639 2647 2649 2653 2660 2666\n",
      " 2667 2672 2675 2684 2689 2692 2705 2711 2715 2719 2720 2729 2734 2739\n",
      " 2770 2774 2791 2808 2815 2817 2824 2826 2828 2832 2836 2841 2843 2851\n",
      " 2856 2860 2867 2872 2881 2883 2890 2903 2905 2912 2931 2933 2937 2946\n",
      " 2950 2952 2954 2955 2959 2966 2968 2970 2977 2980 2985 2986 2995 3000\n",
      " 3004 3012 3013 3016 3019 3020 3025 3031 3032 3035 3038 3047 3053 3111\n",
      " 3125 3126 3128 3132 3139 3142 3150 3152 3155 3156 3163 3171 3172 3173\n",
      " 3174 3181 3183 3186 3188 3189 3195 3196 3198 3203 3205 3208 3215 3216\n",
      " 3223 3226 3236 3243 3246 3254 3257 3279 3290 3294 3295 3300 3301 3318\n",
      " 3323 3325 3334 3348 3349 3351 3355 3371 3376 3389 3401 3406 3408 3411\n",
      " 3424 3430 3435 3451 3454 3455 3458 3462 3463 3467 3487 3496 3500 3503\n",
      " 3517 3523 3527 3529 3536 3582 3597 3617 3620 3623 3647 3653 3671 3677\n",
      " 3680 3685 3711 3717 3718 3720 3722 3724 3730 3733 3741 3762 3763 3769\n",
      " 3778 3785 3786 3793 3794 3803 3807 3826 3829 3839 3855 3856 3857 3875\n",
      " 3879 3892 3893 3899 3904 3917 3924 3927 3938 3940 3947 3948 3949 3951\n",
      " 3955 3958 3960 3978 3979 3990 3999 4002 4006 4008 4023 4025 4038 4039\n",
      " 4041 4047 4048 4049 4050 4055 4062 4069 4070 4092 4093 4098 4099 4100\n",
      " 4102 4104 4108 4117 4122 4126 4141 4144 4154 4156 4166 4172 4173 4176\n",
      " 4189 4191 4200 4211 4220 4224 4225 4229 4247 4254 4256 4261 4262 4268\n",
      " 4276 4279 4281 4289 4313 4316 4318 4326 4327 4332 4344 4355 4366 4367\n",
      " 4369 4370 4373 4375 4380 4395 4416 4419 4428 4431 4433 4434 4442 4454\n",
      " 4464 4467 4470 4474 4475 4478 4484 4507 4515 4516 4519 4528 4530 4537\n",
      " 4547 4552 4557 4559 4562 4592 4598 4605 4610 4624 4630 4639 4641 4656\n",
      " 4664 4665 4667 4670 4671 4678 4680 4684 4685 4686 4692 4699 4703 4705\n",
      " 4707 4712 4714 4738 4753 4754 4762 4763 4767 4768 4772 4788 4789 4794\n",
      " 4804 4826 4827 4831 4845 4853 4856 4857 4860 4872 4875 4883 4887 4892\n",
      " 4897 4905 4906 4908 4910 4916 4922 4932 4936 4940 4943 4946 4947 4951\n",
      " 4967 4968 4970 4977 4986 4987 4988 4997 5010 5014 5016 5025 5029 5037\n",
      " 5061 5071 5074 5083 5101 5108 5116 5121 5123 5125 5134 5150 5152 5157\n",
      " 5161 5169 5172 5174 5175 5183 5193 5205 5220 5227 5231 5232 5239 5243\n",
      " 5244 5265 5266 5272 5277 5282 5292 5293 5300 5313 5321 5331 5364 5370\n",
      " 5374 5378 5380 5381 5387 5389 5392 5394 5407 5408 5412 5415 5432 5434\n",
      " 5435 5440 5441 5447 5450 5464 5479 5481 5487 5501 5503 5508 5511 5520\n",
      " 5527 5528 5534 5543 5558 5560 5572 5576 5577 5581 5585 5589 5593 5595\n",
      " 5606 5608 5626 5627 5632 5633 5637 5645 5647 5655 5667 5674 5684 5692]\n",
      "1\n",
      "[   0    1    2 ... 5706 5708 5710]\n",
      "[   4    7    9   11   27   33   56   58   66   68   83   86   88   98\n",
      "  111  112  117  119  125  127  137  144  152  158  162  166  169  173\n",
      "  179  186  191  193  217  220  223  227  229  234  236  244  254  255\n",
      "  269  270  272  277  278  288  294  298  311  315  323  336  337  341\n",
      "  358  360  365  367  372  375  376  384  401  403  404  405  407  432\n",
      "  445  446  447  449  460  465  478  491  496  500  507  518  537  541\n",
      "  543  561  567  573  574  576  577  585  597  598  601  614  617  625\n",
      "  640  657  660  670  671  675  682  683  685  688  689  691  695  701\n",
      "  712  714  723  729  748  751  758  760  763  766  775  792  793  794\n",
      "  799  801  802  816  817  834  835  841  850  853  856  860  868  877\n",
      "  879  884  889  890  897  906  907  911  926  935  937  940  941  942\n",
      "  952  955  958  970  976  992  994 1010 1016 1019 1031 1034 1037 1039\n",
      " 1040 1046 1048 1049 1050 1053 1054 1068 1070 1085 1086 1095 1097 1099\n",
      " 1105 1109 1116 1118 1125 1142 1147 1151 1160 1163 1166 1174 1183 1188\n",
      " 1194 1200 1209 1214 1222 1227 1238 1241 1260 1265 1267 1269 1278 1289\n",
      " 1291 1294 1302 1303 1305 1308 1309 1318 1329 1348 1350 1352 1354 1356\n",
      " 1377 1380 1385 1387 1397 1400 1408 1409 1416 1420 1440 1441 1443 1456\n",
      " 1458 1468 1469 1474 1483 1484 1491 1498 1499 1502 1507 1510 1512 1523\n",
      " 1528 1533 1536 1540 1543 1546 1550 1551 1555 1567 1581 1596 1598 1601\n",
      " 1602 1604 1612 1614 1627 1632 1643 1660 1663 1664 1665 1666 1669 1677\n",
      " 1694 1696 1698 1703 1704 1719 1734 1738 1740 1746 1749 1752 1758 1760\n",
      " 1768 1771 1774 1778 1794 1810 1823 1830 1831 1832 1837 1838 1839 1840\n",
      " 1842 1850 1851 1869 1875 1879 1884 1885 1889 1891 1897 1899 1903 1906\n",
      " 1907 1916 1917 1920 1923 1924 1932 1933 1939 1949 1953 1966 1970 1972\n",
      " 1975 1977 1980 1991 2006 2008 2009 2013 2022 2030 2031 2036 2042 2043\n",
      " 2051 2055 2075 2080 2086 2097 2103 2108 2111 2125 2128 2135 2136 2151\n",
      " 2158 2164 2165 2189 2199 2207 2222 2238 2243 2246 2264 2265 2267 2271\n",
      " 2276 2280 2282 2284 2290 2295 2299 2301 2303 2308 2311 2319 2324 2325\n",
      " 2326 2336 2337 2343 2359 2363 2370 2375 2383 2386 2390 2395 2399 2406\n",
      " 2416 2418 2421 2431 2432 2441 2445 2457 2466 2472 2478 2487 2488 2491\n",
      " 2494 2498 2501 2509 2520 2525 2531 2536 2551 2552 2553 2556 2567 2580\n",
      " 2583 2585 2587 2595 2597 2600 2605 2629 2635 2638 2640 2642 2657 2658\n",
      " 2659 2665 2673 2674 2676 2679 2680 2682 2690 2699 2703 2708 2717 2733\n",
      " 2740 2747 2748 2752 2756 2757 2786 2803 2804 2811 2821 2822 2823 2825\n",
      " 2827 2838 2847 2854 2861 2866 2877 2878 2884 2886 2887 2894 2913 2918\n",
      " 2923 2939 2941 2942 2953 2975 2984 2987 2992 2993 2994 2996 2997 3026\n",
      " 3028 3044 3045 3058 3060 3062 3063 3069 3074 3075 3078 3084 3085 3088\n",
      " 3093 3094 3095 3100 3101 3115 3121 3130 3145 3153 3154 3161 3162 3164\n",
      " 3165 3166 3176 3182 3185 3191 3197 3199 3200 3201 3209 3214 3224 3227\n",
      " 3232 3238 3247 3249 3250 3266 3269 3275 3284 3311 3314 3321 3329 3336\n",
      " 3343 3360 3366 3368 3377 3390 3391 3394 3399 3416 3440 3442 3444 3471\n",
      " 3474 3484 3486 3488 3493 3495 3501 3507 3511 3526 3532 3533 3535 3538\n",
      " 3543 3545 3547 3556 3560 3566 3568 3573 3578 3580 3587 3590 3598 3601\n",
      " 3602 3603 3605 3607 3614 3615 3626 3634 3636 3651 3658 3659 3663 3682\n",
      " 3686 3687 3694 3695 3699 3709 3715 3727 3748 3751 3754 3755 3767 3777\n",
      " 3782 3797 3798 3800 3804 3805 3811 3813 3814 3816 3820 3828 3830 3836\n",
      " 3838 3848 3852 3859 3862 3870 3872 3874 3877 3885 3896 3915 3918 3921\n",
      " 3936 3941 3946 3953 3956 3964 3969 3973 3974 3975 3980 3981 3986 3988\n",
      " 3991 3995 4003 4010 4011 4013 4019 4020 4024 4027 4033 4036 4045 4053\n",
      " 4074 4075 4082 4085 4101 4110 4114 4133 4134 4139 4145 4147 4149 4157\n",
      " 4161 4162 4164 4167 4174 4179 4186 4198 4208 4215 4235 4250 4251 4274\n",
      " 4280 4290 4298 4314 4336 4343 4349 4356 4357 4363 4365 4368 4382 4385\n",
      " 4388 4407 4408 4410 4411 4418 4436 4441 4443 4448 4459 4488 4489 4491\n",
      " 4492 4499 4504 4513 4522 4529 4531 4535 4544 4553 4554 4560 4563 4567\n",
      " 4571 4574 4575 4578 4579 4580 4589 4603 4607 4614 4616 4620 4621 4626\n",
      " 4634 4638 4644 4645 4647 4649 4652 4672 4674 4679 4681 4704 4706 4708\n",
      " 4711 4715 4719 4723 4731 4736 4740 4742 4743 4749 4758 4766 4773 4777\n",
      " 4779 4782 4783 4785 4791 4797 4800 4808 4812 4815 4816 4823 4834 4841\n",
      " 4842 4863 4864 4874 4876 4880 4885 4888 4891 4912 4914 4929 4931 4941\n",
      " 4942 4952 4953 4963 4964 4971 4979 4982 4984 4993 4994 4995 5004 5009\n",
      " 5017 5022 5047 5048 5050 5052 5057 5066 5081 5087 5092 5094 5099 5118\n",
      " 5135 5140 5185 5197 5199 5200 5209 5228 5229 5230 5249 5250 5251 5256\n",
      " 5258 5274 5281 5290 5295 5297 5299 5309 5310 5318 5325 5327 5328 5339\n",
      " 5345 5347 5349 5353 5359 5362 5366 5373 5390 5398 5401 5403 5406 5410\n",
      " 5416 5418 5422 5425 5431 5444 5449 5458 5459 5461 5469 5472 5473 5480\n",
      " 5489 5496 5513 5514 5516 5522 5523 5544 5545 5552 5562 5566 5567 5570\n",
      " 5575 5583 5584 5597 5600 5602 5629 5630 5635 5640 5641 5644 5646 5653\n",
      " 5662 5665 5668 5673 5682 5685 5691 5693 5699 5700 5701 5707 5709 5711]\n",
      "2\n",
      "[   1    2    3 ... 5709 5710 5711]\n",
      "[   0    6   26   41   45   46   50   55   57   61   64   67   78   80\n",
      "   81   90   93   96   99  104  106  110  120  123  126  151  153  157\n",
      "  164  176  187  200  211  218  237  241  250  251  257  258  265  268\n",
      "  275  276  280  283  287  289  300  302  304  305  322  345  349  369\n",
      "  373  374  390  398  399  419  433  434  442  456  469  470  481  485\n",
      "  494  508  511  512  514  517  523  529  535  538  540  542  544  550\n",
      "  551  552  556  581  584  588  593  615  626  639  641  647  648  658\n",
      "  661  668  669  673  676  686  690  692  716  718  724  725  727  731\n",
      "  733  739  741  743  744  745  749  753  755  765  781  784  785  791\n",
      "  800  805  807  808  811  814  826  827  831  832  838  843  855  858\n",
      "  876  880  883  886  893  896  901  903  905  909  914  918  933  946\n",
      "  953  963  983  987  989  991 1000 1001 1012 1022 1024 1026 1029 1036\n",
      " 1056 1057 1058 1062 1071 1078 1083 1087 1090 1092 1098 1100 1108 1110\n",
      " 1128 1130 1131 1134 1135 1137 1141 1145 1152 1158 1161 1169 1177 1185\n",
      " 1190 1204 1206 1245 1254 1256 1257 1258 1266 1268 1271 1275 1283 1299\n",
      " 1319 1330 1337 1343 1344 1347 1351 1362 1382 1388 1392 1396 1406 1414\n",
      " 1415 1422 1433 1436 1444 1448 1450 1455 1463 1465 1476 1480 1497 1501\n",
      " 1503 1505 1515 1521 1526 1527 1529 1531 1535 1553 1559 1563 1569 1573\n",
      " 1576 1577 1584 1587 1588 1590 1617 1618 1620 1622 1623 1628 1630 1631\n",
      " 1635 1639 1641 1644 1654 1675 1678 1680 1684 1687 1688 1693 1701 1702\n",
      " 1709 1721 1723 1736 1742 1756 1763 1767 1775 1784 1789 1790 1792 1798\n",
      " 1799 1804 1805 1808 1815 1819 1825 1834 1835 1844 1845 1847 1848 1857\n",
      " 1873 1881 1892 1894 1909 1913 1925 1934 1944 1945 1952 1967 1973 1982\n",
      " 1994 1996 2005 2007 2015 2028 2034 2038 2044 2050 2056 2064 2066 2072\n",
      " 2073 2078 2081 2088 2092 2093 2095 2100 2106 2110 2114 2116 2117 2121\n",
      " 2131 2133 2134 2139 2144 2149 2150 2152 2156 2168 2171 2173 2181 2182\n",
      " 2192 2197 2204 2211 2244 2262 2268 2272 2279 2281 2291 2292 2293 2307\n",
      " 2310 2317 2322 2328 2339 2344 2352 2354 2357 2365 2366 2368 2369 2382\n",
      " 2392 2403 2409 2423 2427 2438 2449 2451 2452 2461 2473 2483 2489 2493\n",
      " 2497 2499 2508 2510 2512 2516 2521 2527 2533 2534 2535 2543 2549 2566\n",
      " 2569 2571 2575 2579 2581 2588 2591 2592 2599 2606 2615 2619 2641 2646\n",
      " 2664 2669 2695 2696 2697 2702 2706 2716 2726 2730 2735 2751 2753 2755\n",
      " 2767 2771 2776 2777 2782 2788 2792 2805 2814 2819 2820 2830 2839 2846\n",
      " 2852 2855 2859 2864 2888 2893 2909 2911 2915 2919 2921 2948 2951 2956\n",
      " 2961 2963 2964 2965 2967 2979 2981 2982 2991 3003 3006 3008 3014 3017\n",
      " 3039 3049 3055 3059 3066 3072 3073 3077 3086 3096 3099 3106 3107 3110\n",
      " 3112 3113 3116 3120 3133 3136 3159 3160 3168 3170 3175 3187 3202 3204\n",
      " 3206 3207 3212 3225 3231 3234 3248 3251 3256 3259 3264 3267 3280 3282\n",
      " 3283 3288 3289 3292 3296 3305 3307 3308 3315 3316 3331 3340 3345 3353\n",
      " 3357 3364 3367 3384 3388 3395 3397 3400 3409 3417 3418 3419 3422 3427\n",
      " 3431 3433 3439 3453 3461 3468 3469 3475 3477 3478 3482 3489 3499 3515\n",
      " 3530 3534 3552 3555 3558 3559 3567 3569 3571 3581 3589 3592 3612 3629\n",
      " 3637 3638 3640 3641 3648 3649 3657 3660 3664 3667 3673 3679 3684 3689\n",
      " 3690 3691 3703 3704 3721 3731 3738 3753 3760 3761 3766 3775 3779 3780\n",
      " 3789 3790 3801 3817 3818 3819 3821 3822 3831 3832 3833 3840 3843 3844\n",
      " 3846 3854 3861 3865 3868 3878 3884 3886 3888 3889 3897 3898 3903 3909\n",
      " 3912 3916 3920 3923 3933 3944 3945 3963 3965 3972 3983 3984 3992 3994\n",
      " 4000 4004 4005 4021 4026 4029 4032 4037 4042 4051 4056 4059 4065 4067\n",
      " 4072 4087 4091 4111 4113 4116 4130 4132 4135 4140 4155 4160 4168 4169\n",
      " 4170 4177 4178 4181 4184 4193 4199 4202 4222 4236 4238 4239 4242 4243\n",
      " 4246 4273 4282 4283 4287 4294 4297 4307 4308 4319 4320 4328 4329 4331\n",
      " 4333 4337 4339 4340 4347 4351 4372 4381 4396 4406 4412 4413 4417 4420\n",
      " 4421 4425 4426 4438 4439 4440 4445 4452 4455 4461 4462 4471 4472 4482\n",
      " 4483 4486 4487 4490 4493 4502 4509 4510 4511 4514 4536 4543 4556 4558\n",
      " 4566 4570 4581 4585 4587 4606 4622 4633 4636 4637 4654 4657 4661 4663\n",
      " 4669 4673 4675 4683 4696 4697 4698 4702 4716 4718 4721 4724 4727 4728\n",
      " 4732 4741 4744 4759 4760 4771 4778 4786 4793 4798 4803 4810 4813 4818\n",
      " 4820 4825 4833 4839 4849 4850 4873 4881 4890 4894 4900 4901 4919 4923\n",
      " 4926 4933 4934 4939 4950 4954 4956 4959 4962 4978 4990 4991 4999 5001\n",
      " 5005 5015 5023 5028 5031 5042 5045 5046 5049 5055 5069 5072 5084 5085\n",
      " 5088 5100 5102 5109 5110 5112 5114 5115 5117 5130 5146 5147 5156 5158\n",
      " 5163 5167 5168 5171 5173 5176 5178 5182 5191 5211 5212 5246 5252 5254\n",
      " 5255 5259 5260 5269 5271 5276 5279 5289 5306 5311 5329 5333 5335 5343\n",
      " 5354 5371 5375 5376 5383 5384 5391 5393 5395 5404 5405 5411 5426 5428\n",
      " 5436 5437 5438 5448 5452 5453 5456 5460 5465 5466 5468 5475 5477 5482\n",
      " 5483 5485 5490 5493 5495 5500 5512 5517 5521 5526 5530 5533 5535 5537\n",
      " 5538 5539 5551 5553 5559 5564 5571 5573 5580 5591 5601 5609 5619 5623\n",
      " 5624 5634 5639 5643 5649 5652 5656 5657 5658 5678 5703 5704 5705 5706]\n",
      "3\n",
      "[   0    1    2 ... 5707 5709 5711]\n",
      "[   8   12   14   15   16   17   19   20   23   31   32   37   39   59\n",
      "   60   63   69   71   73   84   89   91  103  113  132  138  142  148\n",
      "  150  156  160  163  170  199  201  202  215  219  222  230  232  235\n",
      "  239  240  246  259  271  273  274  284  293  296  301  306  312  316\n",
      "  319  320  325  326  330  332  342  343  346  355  362  368  382  383\n",
      "  385  386  389  393  409  416  417  422  424  427  428  454  458  466\n",
      "  479  483  493  501  513  530  539  564  569  575  580  583  587  589\n",
      "  590  591  620  622  651  656  659  663  667  672  684  693  699  703\n",
      "  705  707  711  713  730  734  746  752  759  761  768  769  770  776\n",
      "  777  782  783  804  806  815  821  823  828  833  844  852  865  869\n",
      "  878  895  910  913  915  939  948  949  954  956  957  966  967  968\n",
      "  974  980  988  997 1011 1014 1025 1033 1041 1044 1051 1059 1063 1066\n",
      " 1067 1069 1076 1081 1088 1093 1094 1102 1107 1119 1123 1133 1138 1140\n",
      " 1143 1146 1155 1156 1157 1164 1171 1181 1182 1189 1191 1192 1193 1201\n",
      " 1207 1211 1212 1213 1218 1220 1223 1228 1242 1244 1248 1251 1262 1270\n",
      " 1272 1276 1280 1281 1290 1295 1313 1323 1335 1340 1359 1363 1369 1370\n",
      " 1371 1378 1379 1384 1386 1390 1398 1411 1426 1427 1428 1430 1446 1457\n",
      " 1461 1466 1467 1487 1492 1493 1509 1511 1516 1518 1522 1524 1541 1542\n",
      " 1548 1574 1580 1586 1591 1594 1595 1599 1608 1609 1619 1625 1636 1645\n",
      " 1646 1647 1653 1658 1661 1668 1670 1672 1673 1674 1676 1679 1681 1691\n",
      " 1692 1695 1699 1708 1713 1714 1715 1722 1724 1729 1731 1739 1741 1744\n",
      " 1750 1757 1759 1761 1770 1777 1779 1788 1791 1797 1801 1802 1821 1829\n",
      " 1853 1859 1861 1867 1877 1878 1888 1890 1898 1902 1910 1911 1912 1927\n",
      " 1936 1950 1961 1971 1974 1988 1992 2000 2001 2011 2018 2019 2023 2039\n",
      " 2046 2048 2062 2063 2079 2090 2094 2096 2098 2102 2113 2120 2123 2132\n",
      " 2137 2143 2145 2153 2170 2179 2180 2198 2200 2201 2205 2218 2221 2227\n",
      " 2228 2229 2239 2247 2253 2254 2270 2285 2294 2297 2304 2306 2313 2320\n",
      " 2321 2329 2340 2356 2358 2360 2361 2371 2372 2376 2380 2384 2385 2388\n",
      " 2404 2408 2414 2417 2422 2437 2444 2447 2469 2475 2481 2482 2484 2490\n",
      " 2492 2496 2504 2517 2532 2538 2539 2540 2541 2564 2578 2582 2598 2610\n",
      " 2613 2622 2625 2628 2632 2643 2654 2655 2662 2668 2671 2677 2686 2687\n",
      " 2698 2701 2712 2714 2725 2738 2746 2760 2778 2779 2787 2790 2810 2812\n",
      " 2818 2837 2840 2848 2850 2858 2869 2896 2899 2900 2902 2904 2906 2907\n",
      " 2914 2916 2917 2922 2924 2929 2930 2949 2958 2971 2973 2978 2983 2988\n",
      " 2999 3005 3018 3029 3030 3040 3041 3050 3051 3052 3056 3057 3061 3064\n",
      " 3071 3076 3081 3087 3098 3105 3109 3118 3119 3123 3129 3131 3137 3141\n",
      " 3146 3147 3157 3167 3177 3192 3210 3211 3213 3217 3218 3220 3221 3222\n",
      " 3228 3235 3237 3239 3242 3244 3245 3253 3261 3268 3287 3293 3302 3303\n",
      " 3304 3306 3313 3322 3326 3328 3332 3339 3350 3356 3362 3363 3379 3381\n",
      " 3386 3392 3396 3398 3404 3407 3412 3413 3414 3429 3436 3437 3449 3450\n",
      " 3459 3465 3470 3473 3491 3494 3502 3509 3512 3514 3516 3525 3537 3542\n",
      " 3549 3551 3553 3557 3561 3562 3579 3585 3586 3588 3608 3625 3627 3628\n",
      " 3631 3633 3635 3645 3656 3662 3670 3674 3688 3692 3696 3697 3698 3702\n",
      " 3710 3732 3739 3740 3744 3752 3756 3759 3784 3792 3795 3799 3810 3812\n",
      " 3815 3835 3841 3842 3845 3849 3850 3858 3863 3871 3880 3901 3907 3911\n",
      " 3913 3914 3919 3922 3929 3935 3950 3959 3961 3970 3976 3977 3989 3998\n",
      " 4007 4012 4015 4018 4022 4031 4043 4044 4058 4063 4066 4068 4071 4077\n",
      " 4081 4083 4089 4090 4095 4109 4112 4115 4127 4131 4136 4142 4150 4158\n",
      " 4180 4185 4192 4197 4201 4212 4214 4216 4217 4219 4221 4230 4233 4234\n",
      " 4237 4241 4244 4249 4252 4255 4257 4258 4260 4263 4265 4269 4271 4277\n",
      " 4286 4291 4292 4295 4301 4311 4321 4322 4323 4330 4346 4353 4354 4359\n",
      " 4360 4361 4362 4364 4379 4384 4389 4392 4394 4399 4402 4405 4409 4415\n",
      " 4422 4423 4430 4446 4449 4456 4457 4480 4481 4495 4498 4500 4501 4512\n",
      " 4520 4521 4526 4538 4539 4546 4555 4565 4568 4569 4577 4586 4588 4596\n",
      " 4597 4599 4600 4602 4609 4612 4615 4619 4625 4632 4635 4648 4658 4660\n",
      " 4662 4676 4677 4689 4690 4694 4700 4701 4709 4710 4720 4729 4733 4734\n",
      " 4752 4756 4757 4764 4769 4770 4796 4799 4802 4805 4809 4811 4821 4828\n",
      " 4830 4835 4838 4843 4844 4859 4861 4865 4866 4869 4870 4877 4886 4893\n",
      " 4895 4904 4909 4911 4921 4937 4938 4949 4955 4965 4966 4973 4975 4976\n",
      " 4992 4998 5011 5012 5019 5026 5034 5040 5041 5053 5063 5064 5070 5073\n",
      " 5076 5077 5079 5082 5086 5095 5104 5113 5129 5133 5137 5139 5143 5155\n",
      " 5162 5164 5165 5166 5177 5181 5186 5188 5189 5192 5195 5196 5201 5221\n",
      " 5222 5226 5233 5236 5245 5253 5262 5263 5286 5301 5303 5305 5307 5315\n",
      " 5319 5330 5334 5336 5344 5350 5351 5352 5357 5360 5361 5365 5369 5372\n",
      " 5379 5382 5385 5388 5396 5397 5399 5413 5420 5433 5454 5455 5457 5462\n",
      " 5463 5467 5478 5484 5502 5531 5532 5540 5546 5547 5548 5550 5555 5556\n",
      " 5557 5568 5569 5587 5590 5592 5599 5607 5611 5615 5617 5618 5620 5621\n",
      " 5622 5631 5638 5663 5675 5676 5677 5680 5687 5688 5689 5697 5708 5710]\n",
      "4\n",
      "[   0    1    3 ... 5709 5710 5711]\n",
      "[   2   18   21   24   25   29   30   35   36   40   54   62   70   72\n",
      "   74   76   79   82   85   95  109  116  122  136  141  147  154  174\n",
      "  180  184  189  194  196  198  203  207  208  213  214  224  226  233\n",
      "  238  247  248  252  256  261  262  264  266  279  281  285  292  317\n",
      "  321  327  328  331  333  338  340  348  352  357  364  370  371  378\n",
      "  379  380  388  392  394  400  412  415  423  425  426  429  430  435\n",
      "  436  439  452  453  455  461  471  472  474  475  477  486  488  492\n",
      "  502  506  510  516  519  522  526  532  533  546  553  562  565  571\n",
      "  572  579  582  586  592  594  603  604  609  619  630  631  638  644\n",
      "  646  649  650  653  662  674  677  678  681  694  697  698  700  704\n",
      "  721  726  735  737  738  740  754  762  764  772  789  798  820  842\n",
      "  845  847  859  881  900  904  917  922  924  930  936  943  945  947\n",
      "  950  951  972  973  979  984  986  990  999 1009 1013 1015 1023 1032\n",
      " 1035 1045 1065 1072 1073 1074 1077 1080 1082 1091 1096 1111 1114 1120\n",
      " 1124 1136 1149 1154 1159 1175 1180 1184 1187 1196 1202 1208 1216 1217\n",
      " 1229 1235 1236 1243 1247 1250 1253 1263 1264 1277 1279 1292 1298 1300\n",
      " 1304 1306 1307 1314 1315 1316 1332 1366 1368 1376 1391 1394 1404 1405\n",
      " 1412 1432 1434 1438 1439 1445 1449 1451 1452 1453 1464 1473 1481 1486\n",
      " 1490 1500 1504 1506 1519 1537 1538 1545 1557 1568 1571 1575 1583 1593\n",
      " 1600 1611 1626 1629 1637 1642 1648 1650 1652 1655 1656 1659 1685 1690\n",
      " 1705 1727 1732 1735 1743 1745 1748 1753 1754 1755 1766 1772 1781 1782\n",
      " 1783 1785 1787 1793 1800 1824 1836 1841 1855 1858 1860 1863 1866 1868\n",
      " 1882 1886 1887 1893 1914 1915 1921 1929 1948 1957 1958 1962 1964 1976\n",
      " 1985 1997 1998 1999 2002 2003 2014 2016 2020 2029 2032 2041 2045 2049\n",
      " 2057 2060 2061 2065 2068 2076 2083 2089 2099 2105 2109 2122 2140 2141\n",
      " 2147 2154 2163 2167 2174 2184 2188 2194 2203 2206 2208 2209 2215 2225\n",
      " 2232 2235 2241 2248 2249 2251 2255 2260 2266 2274 2277 2278 2286 2288\n",
      " 2300 2323 2341 2345 2351 2373 2374 2377 2381 2397 2398 2401 2405 2407\n",
      " 2413 2420 2424 2434 2435 2439 2446 2450 2454 2456 2460 2464 2471 2474\n",
      " 2480 2502 2506 2507 2513 2529 2537 2542 2557 2558 2562 2568 2572 2573\n",
      " 2593 2603 2607 2611 2612 2617 2618 2621 2637 2650 2651 2652 2663 2670\n",
      " 2678 2681 2683 2693 2709 2718 2722 2731 2736 2742 2744 2745 2749 2758\n",
      " 2761 2762 2765 2766 2769 2773 2780 2781 2785 2789 2793 2794 2799 2802\n",
      " 2807 2813 2816 2831 2834 2842 2849 2857 2863 2865 2871 2882 2889 2891\n",
      " 2892 2898 2901 2908 2920 2928 2934 2936 2938 2940 2943 2944 2945 2947\n",
      " 2989 2990 3001 3002 3010 3021 3034 3043 3054 3065 3067 3070 3079 3080\n",
      " 3082 3083 3089 3090 3091 3092 3097 3104 3108 3117 3124 3135 3138 3143\n",
      " 3148 3149 3178 3184 3194 3219 3240 3241 3252 3270 3271 3272 3273 3274\n",
      " 3277 3278 3285 3286 3291 3312 3317 3319 3320 3324 3327 3333 3335 3337\n",
      " 3342 3344 3354 3365 3372 3374 3380 3382 3383 3402 3415 3420 3421 3423\n",
      " 3426 3441 3443 3445 3447 3456 3457 3464 3472 3479 3483 3485 3490 3492\n",
      " 3498 3505 3508 3518 3519 3520 3521 3522 3541 3548 3554 3565 3570 3572\n",
      " 3574 3575 3576 3577 3583 3584 3591 3595 3604 3606 3610 3613 3616 3618\n",
      " 3621 3622 3624 3630 3639 3644 3652 3655 3661 3666 3678 3693 3700 3705\n",
      " 3707 3714 3728 3729 3737 3743 3745 3746 3757 3764 3770 3773 3774 3787\n",
      " 3796 3802 3808 3823 3824 3825 3860 3864 3867 3869 3876 3881 3882 3883\n",
      " 3890 3894 3895 3900 3906 3908 3926 3931 3932 3934 3937 3939 3942 3952\n",
      " 3954 3962 3985 3987 3993 4009 4014 4016 4017 4034 4046 4052 4064 4073\n",
      " 4079 4080 4094 4096 4103 4105 4106 4120 4123 4128 4138 4151 4159 4163\n",
      " 4165 4175 4182 4183 4190 4195 4196 4204 4206 4213 4218 4226 4231 4232\n",
      " 4240 4259 4264 4266 4267 4272 4275 4293 4300 4306 4310 4312 4315 4317\n",
      " 4334 4342 4348 4358 4371 4377 4378 4387 4397 4400 4403 4414 4424 4427\n",
      " 4429 4435 4437 4447 4450 4451 4463 4465 4479 4485 4494 4496 4497 4505\n",
      " 4506 4508 4517 4518 4524 4527 4532 4534 4541 4542 4548 4549 4550 4576\n",
      " 4583 4590 4591 4593 4594 4601 4608 4611 4613 4627 4643 4646 4651 4653\n",
      " 4659 4668 4682 4688 4691 4693 4713 4717 4725 4726 4730 4739 4746 4747\n",
      " 4748 4765 4775 4776 4780 4801 4817 4819 4822 4829 4846 4848 4851 4855\n",
      " 4862 4867 4868 4871 4879 4882 4884 4889 4898 4903 4907 4913 4915 4917\n",
      " 4918 4928 4944 4948 4957 4958 4961 4969 4974 4983 4985 4989 4996 5000\n",
      " 5002 5006 5007 5008 5018 5030 5032 5033 5035 5039 5044 5051 5056 5059\n",
      " 5062 5068 5075 5080 5089 5090 5091 5093 5096 5098 5111 5119 5128 5131\n",
      " 5132 5138 5141 5142 5144 5149 5153 5159 5179 5180 5187 5202 5203 5204\n",
      " 5206 5208 5213 5214 5215 5216 5219 5223 5224 5235 5240 5242 5248 5257\n",
      " 5261 5264 5268 5270 5275 5278 5280 5284 5285 5287 5291 5296 5298 5302\n",
      " 5304 5308 5312 5314 5322 5323 5324 5337 5338 5340 5348 5358 5400 5414\n",
      " 5417 5419 5421 5423 5429 5430 5439 5443 5451 5486 5488 5491 5494 5497\n",
      " 5498 5499 5505 5506 5509 5510 5515 5519 5524 5529 5541 5561 5563 5574\n",
      " 5578 5582 5594 5603 5610 5612 5625 5650 5651 5661 5664 5694 5695 5702]\n",
      "5\n",
      "[   0    2    4 ... 5709 5710 5711]\n",
      "[   1    3    5   10   13   22   47   52   53   94   97  101  107  108\n",
      "  114  115  118  121  124  128  129  135  143  145  146  149  159  165\n",
      "  167  172  175  177  181  183  188  192  205  206  209  210  212  221\n",
      "  225  231  242  243  245  249  263  282  286  290  295  308  309  314\n",
      "  324  334  350  351  353  354  359  381  391  395  396  406  411  418\n",
      "  421  431  437  438  440  457  459  462  463  473  482  484  490  495\n",
      "  503  504  505  509  520  521  527  531  536  547  554  555  557  559\n",
      "  560  570  578  596  599  602  607  608  611  613  618  621  623  628\n",
      "  633  642  643  645  652  654  655  664  687  696  709  715  717  719\n",
      "  728  736  742  747  757  778  779  787  788  790  795  809  818  819\n",
      "  824  829  837  839  840  849  851  854  857  863  864  870  872  874\n",
      "  882  885  887  898  902  908  916  920  927  928  931  959  962  969\n",
      "  978  982  995 1008 1020 1027 1030 1038 1042 1043 1064 1101 1103 1127\n",
      " 1129 1132 1148 1153 1162 1170 1176 1186 1195 1197 1199 1203 1205 1219\n",
      " 1221 1224 1226 1230 1232 1233 1237 1240 1246 1252 1259 1261 1284 1286\n",
      " 1293 1296 1297 1312 1317 1320 1321 1328 1331 1339 1341 1353 1355 1364\n",
      " 1365 1374 1389 1393 1395 1399 1401 1402 1410 1417 1418 1421 1423 1424\n",
      " 1429 1431 1435 1442 1454 1459 1460 1470 1471 1472 1475 1479 1485 1488\n",
      " 1496 1513 1514 1520 1525 1532 1534 1539 1544 1547 1554 1556 1560 1562\n",
      " 1570 1578 1579 1582 1585 1592 1597 1603 1605 1606 1607 1610 1613 1615\n",
      " 1616 1633 1638 1657 1662 1667 1671 1682 1686 1689 1697 1710 1711 1716\n",
      " 1717 1726 1728 1730 1733 1762 1796 1809 1813 1816 1817 1818 1822 1826\n",
      " 1833 1846 1849 1852 1862 1865 1870 1874 1876 1883 1895 1904 1908 1922\n",
      " 1926 1931 1937 1940 1941 1942 1943 1946 1951 1954 1955 1956 1979 1981\n",
      " 1983 1986 1987 1989 1995 2004 2010 2012 2017 2024 2025 2033 2035 2037\n",
      " 2040 2052 2058 2059 2069 2071 2077 2084 2085 2087 2091 2112 2115 2124\n",
      " 2129 2130 2142 2155 2162 2172 2175 2177 2178 2186 2195 2202 2212 2220\n",
      " 2223 2230 2231 2236 2237 2240 2256 2259 2273 2283 2289 2296 2298 2305\n",
      " 2316 2330 2332 2348 2350 2353 2364 2367 2391 2396 2400 2411 2425 2426\n",
      " 2440 2442 2443 2453 2458 2463 2467 2468 2479 2495 2505 2511 2514 2519\n",
      " 2522 2523 2524 2546 2547 2554 2555 2559 2561 2576 2584 2596 2609 2623\n",
      " 2624 2626 2627 2630 2631 2644 2645 2648 2656 2661 2685 2688 2691 2694\n",
      " 2700 2704 2707 2710 2713 2721 2723 2724 2727 2728 2732 2737 2741 2743\n",
      " 2750 2754 2759 2763 2764 2768 2772 2775 2783 2784 2795 2796 2797 2798\n",
      " 2800 2801 2806 2809 2829 2833 2835 2844 2845 2853 2862 2868 2870 2873\n",
      " 2874 2875 2876 2879 2880 2885 2895 2897 2910 2925 2926 2927 2932 2935\n",
      " 2957 2960 2962 2969 2972 2974 2976 2998 3007 3009 3011 3015 3022 3023\n",
      " 3024 3027 3033 3036 3037 3042 3046 3048 3068 3102 3103 3114 3122 3127\n",
      " 3134 3140 3144 3151 3158 3169 3179 3180 3190 3193 3229 3230 3233 3255\n",
      " 3258 3260 3262 3263 3265 3276 3281 3297 3298 3299 3309 3310 3330 3338\n",
      " 3341 3346 3347 3352 3358 3359 3361 3369 3370 3373 3375 3378 3385 3387\n",
      " 3393 3403 3405 3410 3425 3428 3432 3434 3438 3446 3448 3452 3460 3466\n",
      " 3476 3480 3481 3497 3504 3506 3510 3513 3524 3528 3531 3539 3540 3544\n",
      " 3546 3550 3563 3564 3593 3594 3596 3599 3600 3609 3611 3619 3632 3642\n",
      " 3643 3646 3650 3654 3665 3668 3669 3672 3675 3676 3681 3683 3701 3706\n",
      " 3708 3712 3713 3716 3719 3723 3725 3726 3734 3735 3736 3742 3747 3749\n",
      " 3750 3758 3765 3768 3771 3772 3776 3781 3783 3788 3791 3806 3809 3827\n",
      " 3834 3837 3847 3851 3853 3866 3873 3887 3891 3902 3905 3910 3925 3928\n",
      " 3930 3943 3957 3966 3967 3968 3971 3982 3996 3997 4001 4028 4030 4035\n",
      " 4040 4054 4057 4060 4061 4076 4078 4084 4086 4088 4097 4107 4118 4119\n",
      " 4121 4124 4125 4129 4137 4143 4146 4148 4152 4153 4171 4187 4188 4194\n",
      " 4203 4205 4207 4209 4210 4223 4227 4228 4245 4248 4253 4270 4278 4284\n",
      " 4285 4288 4296 4299 4302 4303 4304 4305 4309 4324 4325 4335 4338 4341\n",
      " 4345 4350 4352 4374 4376 4383 4386 4390 4391 4393 4398 4401 4404 4432\n",
      " 4444 4453 4458 4460 4466 4468 4469 4473 4476 4477 4503 4523 4525 4533\n",
      " 4540 4545 4551 4561 4564 4572 4573 4582 4584 4595 4604 4617 4618 4623\n",
      " 4628 4629 4631 4640 4642 4650 4655 4666 4687 4695 4722 4735 4737 4745\n",
      " 4750 4751 4755 4761 4774 4781 4784 4787 4790 4792 4795 4806 4807 4814\n",
      " 4824 4832 4836 4837 4840 4847 4852 4854 4858 4878 4896 4899 4902 4920\n",
      " 4924 4925 4927 4930 4935 4945 4960 4972 4980 4981 5003 5013 5020 5021\n",
      " 5024 5027 5036 5038 5043 5054 5058 5060 5065 5067 5078 5097 5103 5105\n",
      " 5106 5107 5120 5122 5124 5126 5127 5136 5145 5148 5151 5154 5160 5170\n",
      " 5184 5190 5194 5198 5207 5210 5217 5218 5225 5234 5237 5238 5241 5247\n",
      " 5267 5273 5283 5288 5294 5316 5317 5320 5326 5332 5341 5342 5346 5355\n",
      " 5356 5363 5367 5368 5377 5386 5402 5409 5424 5427 5442 5445 5446 5470\n",
      " 5471 5474 5476 5492 5504 5507 5518 5525 5536 5542 5549 5554 5565 5579\n",
      " 5586 5588 5596 5598 5604 5605 5613 5614 5616 5628 5636 5642 5648 5654\n",
      " 5659 5660 5666 5669 5670 5671 5672 5679 5681 5683 5686 5690 5696 5698]\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_indices, valid_indices) in enumerate(k_fold.split(all_img_names)):\n",
    "    print(fold_idx)\n",
    "    print(train_indices)\n",
    "    print(valid_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ps4364/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "For 0\n",
      "Training Cov-T\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [04:16<7:03:05, 256.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 0 Loss: tensor(1.9725, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [08:35<7:01:02, 257.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 1 Loss: tensor(1.9616, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [12:53<6:57:29, 258.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 2 Loss: tensor(1.9689, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [17:12<6:53:29, 258.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 3 Loss: tensor(1.9479, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [21:31<6:49:37, 258.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 4 Loss: tensor(1.9665, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [25:50<6:45:31, 258.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 5 Loss: tensor(1.9574, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [30:09<6:41:13, 258.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 6 Loss: tensor(1.9592, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [34:28<6:36:41, 258.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 7 Loss: tensor(1.9728, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [38:48<6:33:02, 259.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 8 Loss: tensor(1.9666, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [43:07<6:28:52, 259.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 9 Loss: tensor(1.9604, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [47:27<6:24:33, 259.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 10 Loss: tensor(1.9619, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [51:46<6:20:22, 259.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 11 Loss: tensor(1.9574, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [56:05<6:15:55, 259.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 12 Loss: tensor(1.9582, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [1:00:24<6:11:32, 259.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 13 Loss: tensor(1.9573, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [1:04:44<6:07:35, 259.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 14 Loss: tensor(1.9628, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [1:09:04<6:03:17, 259.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 15 Loss: tensor(1.9345, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [1:13:23<5:58:39, 259.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 16 Loss: tensor(1.9573, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [1:17:42<5:54:12, 259.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 17 Loss: tensor(1.9628, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [1:22:00<5:49:33, 258.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 18 Loss: tensor(1.9625, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [1:26:19<5:45:16, 258.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 19 Loss: tensor(1.9536, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [1:30:38<5:40:58, 258.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 20 Loss: tensor(1.9648, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [1:34:57<5:36:42, 259.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 21 Loss: tensor(1.9461, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [1:39:16<5:32:22, 259.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 22 Loss: tensor(1.9605, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [1:43:35<5:28:03, 258.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 23 Loss: tensor(1.9658, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [1:47:53<5:23:23, 258.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 24 Loss: tensor(1.9569, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [1:52:11<5:18:54, 258.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 25 Loss: tensor(1.9669, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [1:56:30<5:14:26, 258.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 26 Loss: tensor(1.9591, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [2:00:48<5:10:05, 258.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 27 Loss: tensor(1.9592, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [2:05:06<5:05:42, 258.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 28 Loss: tensor(1.9520, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [2:09:25<5:01:30, 258.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 29 Loss: tensor(1.9557, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [2:13:43<4:57:10, 258.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 30 Loss: tensor(1.9552, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [2:18:02<4:52:52, 258.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 31 Loss: tensor(1.9531, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [2:22:20<4:48:33, 258.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 32 Loss: tensor(1.9611, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [2:26:38<4:44:17, 258.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 33 Loss: tensor(1.9462, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [2:30:57<4:39:59, 258.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 34 Loss: tensor(1.9536, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [2:35:15<4:35:41, 258.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 35 Loss: tensor(1.9758, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [2:39:34<4:31:28, 258.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 36 Loss: tensor(1.9606, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [2:43:52<4:27:01, 258.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 37 Loss: tensor(1.9589, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [2:48:11<4:22:49, 258.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 38 Loss: tensor(1.9639, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [2:52:30<4:18:37, 258.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 39 Loss: tensor(1.9715, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [2:56:49<4:14:27, 258.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 40 Loss: tensor(1.9473, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [3:01:08<4:10:10, 258.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 41 Loss: tensor(1.9699, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [3:05:27<4:05:52, 258.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 42 Loss: tensor(1.9643, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [3:09:46<4:01:34, 258.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 43 Loss: tensor(1.9643, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [3:14:04<3:57:16, 258.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 44 Loss: tensor(1.9458, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [3:18:24<3:53:03, 258.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 45 Loss: tensor(1.9561, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [3:22:43<3:48:46, 259.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 46 Loss: tensor(1.9421, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [3:27:02<3:44:27, 258.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 47 Loss: tensor(1.9696, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [3:31:21<3:40:11, 259.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 48 Loss: tensor(1.9624, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [3:35:40<3:35:50, 259.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 49 Loss: tensor(1.9516, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [3:39:59<3:31:35, 259.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 50 Loss: tensor(1.9460, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [3:44:18<3:27:09, 258.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 51 Loss: tensor(1.9652, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [3:48:37<3:22:51, 258.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 52 Loss: tensor(1.9621, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [3:52:56<3:18:30, 258.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 53 Loss: tensor(1.9519, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [3:57:15<3:14:13, 258.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 54 Loss: tensor(1.9614, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [4:01:34<3:09:56, 259.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 55 Loss: tensor(1.9715, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [4:05:53<3:05:37, 259.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 56 Loss: tensor(1.9602, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [4:10:12<3:01:20, 259.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 57 Loss: tensor(1.9618, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [4:14:31<2:57:00, 259.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 58 Loss: tensor(1.9586, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [4:18:50<2:52:38, 258.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 59 Loss: tensor(1.9680, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [4:23:09<2:48:24, 259.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 60 Loss: tensor(1.9526, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [4:27:28<2:44:04, 259.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 61 Loss: tensor(1.9429, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [4:31:47<2:39:42, 258.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 62 Loss: tensor(1.9515, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [4:36:06<2:35:28, 259.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 63 Loss: tensor(1.9759, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [4:40:25<2:31:09, 259.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 64 Loss: tensor(1.9572, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [4:44:45<2:26:51, 259.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 65 Loss: tensor(1.9665, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [4:49:03<2:22:21, 258.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 66 Loss: tensor(1.9420, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [4:53:23<2:18:17, 259.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 67 Loss: tensor(1.9512, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [4:57:41<2:13:45, 258.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 68 Loss: tensor(1.9560, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [5:01:59<2:09:19, 258.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 69 Loss: tensor(1.9647, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [5:06:17<2:04:51, 258.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 70 Loss: tensor(1.9590, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [5:10:35<2:00:30, 258.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 71 Loss: tensor(1.9678, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [5:14:53<1:56:11, 258.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For - 72 Loss: tensor(1.9535, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_indices, valid_indices) in enumerate(k_fold.split(all_img_names)):\n",
    "    model_cnn = timm.create_model(cfg.cnn_name, pretrained=True)\n",
    "    model_vit = timm.create_model(cfg.vit_name, pretrained=True)\n",
    "    model_cnn.to(device)\n",
    "    model_vit.to(device)\n",
    "    print('*'*10)\n",
    "    print('For',fold_idx)\n",
    "    fold_train_img_names = [all_img_names[idx] for idx in train_indices]\n",
    "    fold_valid_img_names = [all_img_names[idx] for idx in valid_indices]\n",
    "    fold_train_img_labels = [all_img_labels_ts[idx] for idx in train_indices]\n",
    "    fold_valid_img_labels = [all_img_labels_ts[idx] for idx in valid_indices]\n",
    "    train_dataset = Dataset(CFG, fold_train_img_names, fold_train_img_labels, train_transform)\n",
    "    valid_dataset = Dataset(CFG, fold_valid_img_names, fold_valid_img_labels, valid_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n",
    "    \n",
    "    #Train SSL\n",
    "    print('Training Cov-T')\n",
    "    ssl_train_model(train_loader,model_vit,criterion_vit,optimizer_vit,scheduler_vit,model_cnn,criterion_cnn,optimizer_cnn,scheduler_cnn,num_epochs=100)\n",
    "    #Saving SSL Models\n",
    "    print('Saving Cov-T')\n",
    "    \n",
    "    torch.save(model_cnn,'/scratch/ps4364/BTMRI/code/cass/cass-r50-{}-bMRI-with-valid.pt'.format(fold_idx))\n",
    "    torch.save(model_vit,'/scratch/ps4364/BTMRI/code/cass/cass-vit-{}-bMRI-with-valid.pt'.format(fold_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_idx, (train_indices, valid_indices) in enumerate(k_fold.split(all_img_names)):\n",
    "    model_vit=torch.load('/scratch/ps4364/BTMRI/code/Cov-T/cov-t/covt-vit-{}-bMRI-with-valid.pt'.format(fold_idx))\n",
    "    model_cnn=torch.load('/scratch/ps4364/BTMRI/code/Cov-T/cov-t/covt-r50-{}-bMRI-with-valid.pt'.format(fold_idx))\n",
    "    last_loss=999999999\n",
    "    val_loss_arr=[]\n",
    "    train_loss_arr=[]\n",
    "    counter=0\n",
    "    \n",
    "    model_cnn.to(device)\n",
    "    model_vit.to(device)\n",
    "    print('*'*10)\n",
    "    print('For',fold_idx)\n",
    "    onep_train_indices = np.random.choice(train_indices, int(len(train_indices)*0.1), replace=False) \n",
    "    fold_train_img_names = [all_img_names[idx] for idx in onep_train_indices]\n",
    "    fold_train_img_labels = [all_img_labels_ts[idx] for idx in onep_train_indices] \n",
    "    fold_valid_img_names = [all_img_names[idx] for idx in valid_indices]\n",
    "    \n",
    "    fold_valid_img_labels = [all_img_labels_ts[idx] for idx in valid_indices]\n",
    "    train_dataset = Dataset(CFG, fold_train_img_names, fold_train_img_labels, train_transform)\n",
    "    valid_dataset = Dataset(CFG, fold_valid_img_names, fold_valid_img_labels, valid_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, num_workers=CFG.num_workers, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.batch_size, shuffle=False, num_workers=CFG.num_workers)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Train Correspong Supervised CNN\n",
    "    print('Fine tunning Cov-T')\n",
    "    writer = SummaryWriter()\n",
    "    model_cnn.fc=nn.Linear(in_features=2048, out_features=4, bias=True)\n",
    "    criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "    metric = MyF1Score(cfg)\n",
    "    val_metric=MyF1Score(cfg)\n",
    "    optimizer = torch.optim.Adam(model_cnn.parameters(), lr = 3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr)\n",
    "    model_cnn.train()\n",
    "    from torch.autograd import Variable\n",
    "    best=0\n",
    "    best_val=0\n",
    "    for epoch in tqdm(range(50)):\n",
    "        total_loss = 0\n",
    "        for images,label in train_loader:\n",
    "            model_cnn.train()\n",
    "            images = images.to(device)\n",
    "            label = label.to(device)\n",
    "            model_cnn.to(device)\n",
    "            pred_ts=model_cnn(images)\n",
    "            loss = criterion(pred_ts, label)\n",
    "            score = metric(pred_ts,label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.detach()\n",
    "        avg_loss=total_loss/ len(train_loader)\n",
    "        train_score=metric.compute()\n",
    "        logs = {'train_loss': avg_loss, 'train_f1': train_score, 'lr': optimizer.param_groups[0]['lr']}\n",
    "        writer.add_scalar(\"CNN Supervised Loss/train\", loss, epoch)\n",
    "        writer.add_scalar(\"CNN Supervised F1/train\", train_score, epoch)\n",
    "        print(logs)\n",
    "        if best < train_score:\n",
    "            best=train_score\n",
    "            model_cnn.eval()\n",
    "            total_loss = 0\n",
    "            for images,label in valid_loader:\n",
    "                images = images.to(device)\n",
    "                label = label.to(device)\n",
    "                model_cnn.to(device)\n",
    "                pred_ts=model_cnn(images)\n",
    "                score_val = val_metric(pred_ts,label)\n",
    "                val_loss = criterion(pred_ts, label)\n",
    "                total_loss += val_loss.detach()\n",
    "            avg_loss=total_loss/ len(train_loader)   \n",
    "            print('Val Loss:',avg_loss)\n",
    "            val_score=val_metric.compute()\n",
    "            print('CNN Validation Score:',val_score)\n",
    "            writer.add_scalar(\"CNN Supervised F1/Validation\", val_score, epoch)\n",
    "            if avg_loss > last_loss:\n",
    "                counter+=1\n",
    "            else:\n",
    "                counter=0\n",
    "                \n",
    "            last_loss = avg_loss\n",
    "            if counter > 5:\n",
    "                print('Early Stopping!')\n",
    "                break\n",
    "            else:\n",
    "                if val_score > best_val:\n",
    "                    best_val=val_score\n",
    "                    print('Saving')\n",
    "                    torch.save(model_cnn,\n",
    "                    '/scratch/ps4364/BTMRI/code/1p-data/Cov-t/covt-r50-label-bMRI-10p-es-{}.pt'.format(fold_idx))\n",
    "    writer.flush()\n",
    "    last_loss=999999999\n",
    "    val_loss_arr=[]\n",
    "    train_loss_arr=[]\n",
    "    counter=0\n",
    "    # Training the Corresponding ViT\n",
    "    model_vit.head=nn.Linear(in_features=768, out_features=4, bias=True)\n",
    "    criterion = FocalLoss(cfg.fl_alpha, cfg.fl_gamma)\n",
    "    metric = MyF1Score(cfg)\n",
    "    optimizer = torch.optim.Adam(model_vit.parameters(), lr = 3e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=cfg.t_max,eta_min=cfg.min_lr)\n",
    "    model_vit.train()\n",
    "    val_metric=MyF1Score(cfg)\n",
    "    writer = SummaryWriter()\n",
    "    from torch.autograd import Variable\n",
    "    best=0\n",
    "    best_val=0\n",
    "    for epoch in tqdm(range(50)):\n",
    "        total_loss = 0\n",
    "        for images,label in train_loader:\n",
    "            model_vit.train()\n",
    "            images = images.to(device)\n",
    "            label = label.to(device)\n",
    "            model_vit.to(device)\n",
    "            pred_ts=model_vit(images)\n",
    "            loss = criterion(pred_ts, label)\n",
    "            score = metric(pred_ts,label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            total_loss += loss.detach()\n",
    "        avg_loss=total_loss/ len(train_loader)\n",
    "        train_score=metric.compute()\n",
    "        logs = {'train_loss': loss, 'train_f1': train_score, 'lr': optimizer.param_groups[0]['lr']}\n",
    "        writer.add_scalar(\"ViT Supervised Loss/train\", loss, epoch)\n",
    "        writer.add_scalar(\"ViT Supervised F1/train\", train_score, epoch)\n",
    "        print(logs)\n",
    "        if best < train_score:\n",
    "            best=train_score\n",
    "            model_vit.eval()\n",
    "            total_loss = 0\n",
    "            for images,label in valid_loader:\n",
    "                images = images.to(device)\n",
    "                label = label.to(device)\n",
    "                model_vit.to(device)\n",
    "                pred_ts=model_vit(images)\n",
    "                score_val = val_metric(pred_ts,label)\n",
    "                val_loss = criterion(pred_ts, label)\n",
    "                total_loss += val_loss.detach()\n",
    "            avg_loss=total_loss/ len(train_loader)\n",
    "            val_score=val_metric.compute()\n",
    "            print('ViT Validation Score:',val_score)\n",
    "            print('Val Loss:',avg_loss)\n",
    "            writer.add_scalar(\"ViT Supervised F1/Validation\", val_score, epoch)\n",
    "            if avg_loss > last_loss:\n",
    "                counter+=1\n",
    "            else:\n",
    "                counter=0\n",
    "                \n",
    "            last_loss = avg_loss\n",
    "            if counter > 5:\n",
    "                print('Early Stopping!')\n",
    "                break\n",
    "            else:\n",
    "                if val_score > best_val:\n",
    "                    best_val=val_score\n",
    "                    print('Saving')\n",
    "                    torch.save(model_vit,\n",
    "                                   '/scratch/ps4364/BTMRI/code/1p-data/Cov-t/covt-vit-bMRI-10p-es-{}.pt'.format(fold_idx))\n",
    "                        \n",
    "        writer.flush()                \n",
    "        print('*'*10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}